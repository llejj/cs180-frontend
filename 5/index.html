<!DOCTYPE html>
<html>
<head>
    <title>Project 5: Diffusion Models</title>
    <style>
        body {
            font-family: 'Segoe UI', Tahoma, Geneva, Verdana, sans-serif;
            line-height: 1.6;
            max-width: 1200px;
            margin: 0 auto;
            padding: 20px;
            background-color: #f5f5f5;
            color: #333;
        }

        h1, h2, h3, h4 {
            color: #2c3e50;
            margin-top: 30px;
        }

        h1 {
            text-align: center;
            border-bottom: 2px solid #2c3e50;
            padding-bottom: 10px;
        }

        .container {
            background-color: white;
            padding: 30px;
            border-radius: 8px;
            box-shadow: 0 2px 4px rgba(0,0,0,0.1);
            margin-bottom: 30px;
        }

        img {
            max-width: 100%;
            height: auto;
            display: block;
            margin: 20px auto;
            border-radius: 4px;
            box-shadow: 0 2px 4px rgba(0,0,0,0.1);
        }

        .image-row {
            display: flex;
            justify-content: center;
            flex-wrap: wrap;
            gap: 15px;
            margin: 20px 0;
        }

        .image-row img {
            flex: 0 1 auto;
            width: 120px;
            height: 120px;
            margin: 0;
            image-rendering: auto;
        }

        .image-row.large img {
            width: 180px;
            height: 180px;
        }

        .image-comparison {
            display: flex;
            justify-content: space-between;
            margin: 20px 0;
            flex-wrap: wrap;
            gap: 20px;
        }

        .image-comparison > div {
            flex: 0 1 48%;
            margin-bottom: 20px;
        }

        .image-comparison img {
            width: 100%;
        }

        .single-image {
            text-align: center;
            margin: 30px 0;
        }

        .single-image img {
            width: 256px;
            height: 256px;
            image-rendering: auto;
        }

        .caption {
            text-align: center;
            font-style: italic;
            margin: 10px 0;
            color: #666;
        }

        code {
            background-color: #f8f9fa;
            padding: 2px 5px;
            border-radius: 3px;
            font-family: 'Courier New', Courier, monospace;
        }

        .section-divider {
            margin: 40px 0;
            border-top: 2px solid #e0e0e0;
        }

        .grid-3 {
            display: grid;
            grid-template-columns: repeat(3, 200px);
            justify-content: center;
            gap: 20px;
            margin: 20px 0;
        }

        .grid-3 img {
            width: 200px;
            height: 200px;
            margin: 0;
            image-rendering: auto;
        }

        .grid-2 {
            display: grid;
            grid-template-columns: repeat(2, 250px);
            justify-content: center;
            gap: 30px;
            margin: 20px 0;
        }

        .grid-2 img {
            width: 250px;
            height: 250px;
            margin: 0;
            image-rendering: auto;
        }
    </style>
</head>
<body>
    <div class="container">
        <h1>Project 5: Fun With Diffusion Models!</h1>

        <h2>Overview</h2>
        <p>
            This project covers diffusion models for image generation. Part A uses the pretrained DeepFloyd IF model,
            and Part B trains a flow matching model on MNIST from scratch.
        </p>

        <div class="section-divider"></div>

        <h2>Part A: The Power of Diffusion Models!</h2>

        <h3>Part 0: Setup</h3>
        <p>
            The project uses DeepFloyd IF, a two-stage diffusion model. Stage 1 generates 64x64 images,
            and Stage 2 upscales to 256x256.
        </p>
        <p>
            <strong>Seed used:</strong> <code>100</code>
        </p>
        <p>
            Sample outputs from the pretrained model with my custom prompts (num_inference_steps=20):
        </p>
        <div class="grid-3">
            <div>
                <img src="media/03___Get_prompt_embeddings_from_the_co_0.png" alt="Cosmic black hole">
                <p class="caption">"a cosmic black hole consuming a swirling galaxy"</p>
            </div>
            <div>
                <img src="media/04___Get_prompt_embeddings_from_the_co_1.png" alt="Cyberpunk alleyway">
                <p class="caption">"a neon-lit cyberpunk alleyway in pouring rain"</p>
            </div>
            <div>
                <img src="media/05___Get_prompt_embeddings_from_the_co_2.png" alt="Floating temple">
                <p class="caption">"a floating ancient temple among sunset clouds"</p>
            </div>
        </div>

        <h4>Effect of num_inference_steps</h4>
        <p>
            Comparing the first prompt with different inference steps:
        </p>
        <div class="grid-2">
            <div>
                <img src="media/00___Get_prompt_embeddings_from_the_co_0.png" alt="20 steps">
                <p class="caption">num_inference_steps=20</p>
            </div>
            <div>
                <img src="media/06___Get_prompt_embeddings_from_the_co_0.png" alt="50 steps">
                <p class="caption">num_inference_steps=50</p>
            </div>
        </div>
        <p>
            <strong>Reflection:</strong> The outputs capture the essence of the prompts well - the black hole shows swirling galactic structures, the cyberpunk alley has neon lighting and rain effects, and the temple floats among clouds. With more inference steps (50 vs 20), the galaxy more clearly displays the "swallowing" of the galaxy by the black hole.
        </p>

        <div class="section-divider"></div>

        <h3>Part 1: Sampling Loops</h3>

        <h4>1.1 Implementing the Forward Process</h4>
        <p>
            The forward process adds noise to a clean image according to the diffusion schedule.
            Given a clean image x<sub>0</sub>, we can obtain a noisy image at timestep t using:
        </p>
        <p style="text-align: center;">
            <code>x_t = sqrt(alpha_bar_t) * x_0 + sqrt(1 - alpha_bar_t) * epsilon</code>
        </p>
        <p>where epsilon is Gaussian noise and alpha_bar_t is the cumulative product of noise schedule alphas.</p>

        <h4>Test Image at Different Noise Levels</h4>
        <div class="image-row large">
            <img src="media/07___Get_test_image_0.png" alt="Original test image">
            <img src="media/09___Show_the_test_image_at_noise_leve_1.png" alt="Noise level 250">
            <img src="media/10___Show_the_test_image_at_noise_leve_2.png" alt="Noise level 500">
            <img src="media/11___Show_the_test_image_at_noise_leve_3.png" alt="Noise level 750">
        </div>
        <p class="caption">Original image, then with noise at t=250, t=500, t=750</p>

        <h4>1.2 Classical Denoising</h4>
        <p>
            Gaussian blur is used as a baseline denoising method.
        </p>
        <div class="image-row large">
            <img src="media/12___Classical_denoising_with_Gaussian_0.png" alt="Noisy at t=250">
            <img src="media/13___Classical_denoising_with_Gaussian_1.png" alt="Blurred t=250">
            <img src="media/14___Classical_denoising_with_Gaussian_2.png" alt="Noisy at t=500">
            <img src="media/15___Classical_denoising_with_Gaussian_3.png" alt="Blurred t=500">
            <img src="media/16___Classical_denoising_with_Gaussian_4.png" alt="Noisy at t=750">
            <img src="media/17___Classical_denoising_with_Gaussian_5.png" alt="Blurred t=750">
        </div>
        <p class="caption">Noisy images and their Gaussian-blurred versions at t=250, 500, 750</p>

        <h4>1.3 One-Step Denoising</h4>
        <p>
            The UNet estimates and removes noise in a single step using a null prompt embedding.
        </p>
        <div class="grid-3">
            <div>
                <img src="media/19___Please_use_the_null_prompt_embedd_1.png" alt="One-step t=250">
                <p class="caption">One-step denoise from t=250</p>
            </div>
            <div>
                <img src="media/22___Please_use_the_null_prompt_embedd_1.png" alt="One-step t=500">
                <p class="caption">One-step denoise from t=500</p>
            </div>
            <div>
                <img src="media/25___Please_use_the_null_prompt_embedd_1.png" alt="One-step t=750">
                <p class="caption">One-step denoise from t=750</p>
            </div>
        </div>

        <h4>1.4 Iterative Denoising</h4>
        <p>
            Iterative denoising removes noise over multiple steps using strided timesteps.
            Starting from timestep 690, we denoise through 30 steps with stride 30.
        </p>
        <div class="image-row">
            <img src="media/27_def_iterative_denoise_im_noisy__i_s_0.png" alt="Step 0">
            <img src="media/28_def_iterative_denoise_im_noisy__i_s_1.png" alt="Step 5">
            <img src="media/29_def_iterative_denoise_im_noisy__i_s_2.png" alt="Step 10">
            <img src="media/30_def_iterative_denoise_im_noisy__i_s_3.png" alt="Step 15">
            <img src="media/31_def_iterative_denoise_im_noisy__i_s_4.png" alt="Step 20">
        </div>
        <p class="caption">Denoising progress at every 5th step</p>

        <div class="image-row large">
            <div style="text-align: center;">
                <img src="media/32_def_iterative_denoise_im_noisy__i_s_0.png" alt="Original">
                <p class="caption">Original</p>
            </div>
            <div style="text-align: center;">
                <img src="media/33_def_iterative_denoise_im_noisy__i_s_1.png" alt="Iterative result">
                <p class="caption">Iterative Denoise</p>
            </div>
            <div style="text-align: center;">
                <img src="media/34_def_iterative_denoise_im_noisy__i_s_2.png" alt="One-step result">
                <p class="caption">One-Step Denoise</p>
            </div>
            <div style="text-align: center;">
                <img src="media/35_def_iterative_denoise_im_noisy__i_s_3.png" alt="Gaussian blur">
                <p class="caption">Gaussian Blur</p>
            </div>
        </div>

        <h4>1.5 Diffusion Model Sampling</h4>
        <p>
            Starting from pure noise (t=990) and iteratively denoising generates new images.
            Using the prompt <code>"a high quality photo"</code>:
        </p>
        <div class="image-row large">
            <img src="media/36___Please_use_this_text_prompt_0.png" alt="Sample 1">
            <img src="media/37___Please_use_this_text_prompt_1.png" alt="Sample 2">
            <img src="media/38___Please_use_this_text_prompt_2.png" alt="Sample 3">
            <img src="media/39___Please_use_this_text_prompt_3.png" alt="Sample 4">
            <img src="media/40___Please_use_this_text_prompt_4.png" alt="Sample 5">
        </div>
        <p class="caption">5 samples generated with "a high quality photo"</p>

        <h4>1.6 Classifier-Free Guidance (CFG)</h4>
        <p>
            CFG combines conditional and unconditional noise estimates:
        </p>
        <p style="text-align: center;">
            <code>epsilon = epsilon_uncond + gamma * (epsilon_cond - epsilon_uncond)</code>
        </p>
        <p>Using gamma=7 with prompt <code>"a high quality photo"</code>:</p>
        <div class="image-row large">
            <img src="media/41___The_conditional_prompt_embedding_0.png" alt="CFG Sample 1">
            <img src="media/42___The_conditional_prompt_embedding_1.png" alt="CFG Sample 2">
            <img src="media/43___The_conditional_prompt_embedding_2.png" alt="CFG Sample 3">
            <img src="media/44___The_conditional_prompt_embedding_3.png" alt="CFG Sample 4">
            <img src="media/45___The_conditional_prompt_embedding_4.png" alt="CFG Sample 5">
        </div>
        <p class="caption">5 samples with CFG (gamma=7) and "a high quality photo"</p>

        <h4>1.7 Image-to-image Translation</h4>
        <p>
            SDEdit adds noise to an existing image and then denoises it. The starting index i_start
            controls the noise level. Using prompt <code>"a high quality photo"</code>:
        </p>

        <h4>SDEdit on Test Image (Campanile)</h4>
        <div class="image-row">
            <img src="media/46___The_conditional_prompt_embedding_0.png" alt="i_start=1">
            <img src="media/47___The_conditional_prompt_embedding_1.png" alt="i_start=3">
            <img src="media/48___The_conditional_prompt_embedding_2.png" alt="i_start=5">
            <img src="media/49___The_conditional_prompt_embedding_3.png" alt="i_start=7">
            <img src="media/50___The_conditional_prompt_embedding_4.png" alt="i_start=10">
            <img src="media/51___The_conditional_prompt_embedding_5.png" alt="i_start=20">
            <img src="media/52___The_conditional_prompt_embedding_6.png" alt="Original">
        </div>
        <p class="caption">SDEdit results with i_start: 1, 3, 5, 7, 10, 20, and original</p>

        <h4>SDEdit on My Own Images</h4>
        <p><strong>Lighthouse:</strong></p>
        <div class="image-row">
            <img src="media/53___SDEdit_on_your_own_image_1__IMG_2_0.png" alt="i_start=1">
            <img src="media/54___SDEdit_on_your_own_image_1__IMG_2_1.png" alt="i_start=3">
            <img src="media/55___SDEdit_on_your_own_image_1__IMG_2_2.png" alt="i_start=5">
            <img src="media/56___SDEdit_on_your_own_image_1__IMG_2_3.png" alt="i_start=7">
            <img src="media/57___SDEdit_on_your_own_image_1__IMG_2_4.png" alt="i_start=10">
            <img src="media/58___SDEdit_on_your_own_image_1__IMG_2_5.png" alt="i_start=20">
            <img src="media/59___SDEdit_on_your_own_image_1__IMG_2_6.png" alt="Original">
        </div>

        <p><strong>Lake:</strong></p>
        <div class="image-row">
            <img src="media/60___SDEdit_on_your_own_image_2__IMG_2_0.png" alt="i_start=1">
            <img src="media/61___SDEdit_on_your_own_image_2__IMG_2_1.png" alt="i_start=3">
            <img src="media/62___SDEdit_on_your_own_image_2__IMG_2_2.png" alt="i_start=5">
            <img src="media/63___SDEdit_on_your_own_image_2__IMG_2_3.png" alt="i_start=7">
            <img src="media/64___SDEdit_on_your_own_image_2__IMG_2_4.png" alt="i_start=10">
            <img src="media/65___SDEdit_on_your_own_image_2__IMG_2_5.png" alt="i_start=20">
            <img src="media/66___SDEdit_on_your_own_image_2__IMG_2_6.png" alt="Original">
        </div>

        <h4>1.7.1 Editing Hand-Drawn and Web Images</h4>
        <p>
            SDEdit applied to non-realistic images. Using prompt <code>"a high quality photo"</code>:
        </p>

        <p><strong>Web Image:</strong></p>
        <div class="image-row">
            <img src="media/68_prompt_embeds___prompt_embeds_dict__0.png" alt="i_start=1">
            <img src="media/69_prompt_embeds___prompt_embeds_dict__1.png" alt="i_start=3">
            <img src="media/70_prompt_embeds___prompt_embeds_dict__2.png" alt="i_start=5">
            <img src="media/71_prompt_embeds___prompt_embeds_dict__3.png" alt="i_start=7">
            <img src="media/72_prompt_embeds___prompt_embeds_dict__4.png" alt="i_start=10">
            <img src="media/73_prompt_embeds___prompt_embeds_dict__5.png" alt="i_start=20">
        </div>

        <p><strong>Hand-Drawn Image 1:</strong></p>
        <div class="image-row">
            <img src="media/74____title_Hand_Drawn_Image_1_0.png" alt="Hand-drawn 1">
        </div>
        <div class="image-row">
            <img src="media/75___Process_Hand_Drawn_Image_1_0.png" alt="i_start=1">
            <img src="media/76___Process_Hand_Drawn_Image_1_1.png" alt="i_start=3">
            <img src="media/77___Process_Hand_Drawn_Image_1_2.png" alt="i_start=5">
            <img src="media/78___Process_Hand_Drawn_Image_1_3.png" alt="i_start=7">
            <img src="media/79___Process_Hand_Drawn_Image_1_4.png" alt="i_start=10">
            <img src="media/80___Process_Hand_Drawn_Image_1_5.png" alt="i_start=20">
        </div>

        <p><strong>Hand-Drawn Image 2:</strong></p>
        <div class="image-row">
            <img src="media/81____title_Hand_Drawn_Image_2_0.png" alt="Hand-drawn 2">
        </div>
        <div class="image-row">
            <img src="media/82___Process_Hand_Drawn_Image_2_0.png" alt="i_start=1">
            <img src="media/83___Process_Hand_Drawn_Image_2_1.png" alt="i_start=3">
            <img src="media/84___Process_Hand_Drawn_Image_2_2.png" alt="i_start=5">
            <img src="media/85___Process_Hand_Drawn_Image_2_3.png" alt="i_start=7">
            <img src="media/86___Process_Hand_Drawn_Image_2_4.png" alt="i_start=10">
            <img src="media/87___Process_Hand_Drawn_Image_2_5.png" alt="i_start=20">
        </div>

        <h4>1.7.2 Inpainting</h4>
        <p>
            Inpainting fills in masked regions. At each denoising step, non-masked regions are
            replaced with the properly noised original.
        </p>

        <p><strong>Campanile:</strong></p>
        <div class="grid-3">
            <div>
                <img src="media/88___Inpainting_the_Campanile_0.png" alt="Original">
                <p class="caption">Original</p>
            </div>
            <div>
                <img src="media/90___Inpainting_the_Campanile_2.png" alt="Mask">
                <p class="caption">Mask</p>
            </div>
            <div>
                <img src="media/93___Inpainting_the_Campanile_2.png" alt="Inpainted">
                <p class="caption">Inpainted</p>
            </div>
        </div>

        <p><strong>Waterfall:</strong></p>
        <div class="grid-3">
            <div>
                <img src="media/94___Inpainting_Image_1__IMG_2709_png__0.png" alt="Original">
                <p class="caption">Original</p>
            </div>
            <div>
                <img src="media/96___Inpainting_Image_1__IMG_2709_png__2.png" alt="Mask">
                <p class="caption">Mask</p>
            </div>
            <div>
                <img src="media/99___Inpainting_Image_1__IMG_2709_png__2.png" alt="Inpainted">
                <p class="caption">Inpainted</p>
            </div>
        </div>

        <p><strong>Bat:</strong></p>
        <div class="grid-3">
            <div>
                <img src="media/100___Inpainting_Image_2__IMG_3146_png__0.png" alt="Original">
                <p class="caption">Original</p>
            </div>
            <div>
                <img src="media/102___Inpainting_Image_2__IMG_3146_png__2.png" alt="Mask">
                <p class="caption">Mask</p>
            </div>
            <div>
                <img src="media/105___Inpainting_Image_2__IMG_3146_png__2.png" alt="Inpainted">
                <p class="caption">Inpainted</p>
            </div>
        </div>

        <h4>1.7.3 Text-Conditional Image-to-image Translation</h4>
        <p>
            SDEdit with text prompts to guide the transformation.
        </p>

        <p><strong>Campanile with prompt "a rocket ship":</strong></p>
        <div class="image-row">
            <img src="media/106___Text_Conditioned_Image_to_image_o_0.png" alt="i_start=1">
            <img src="media/107___Text_Conditioned_Image_to_image_o_1.png" alt="i_start=3">
            <img src="media/108___Text_Conditioned_Image_to_image_o_2.png" alt="i_start=5">
            <img src="media/109___Text_Conditioned_Image_to_image_o_3.png" alt="i_start=7">
            <img src="media/110___Text_Conditioned_Image_to_image_o_4.png" alt="i_start=10">
            <img src="media/111___Text_Conditioned_Image_to_image_o_5.png" alt="i_start=20">
            <img src="media/112___Text_Conditioned_Image_to_image_o_6.png" alt="Original">
        </div>
        <p class="caption">i_start: 1, 3, 5, 7, 10, 20, and original</p>

        <p><strong>Image of city with prompt "a neon-lit cyberpunk alleyway in pouring rain":</strong></p>
        <div class="image-row">
            <img src="media/113___Text_Conditioned_Image_to_image_o_0.png" alt="i_start=1">
            <img src="media/114___Text_Conditioned_Image_to_image_o_1.png" alt="i_start=3">
            <img src="media/115___Text_Conditioned_Image_to_image_o_2.png" alt="i_start=5">
            <img src="media/116___Text_Conditioned_Image_to_image_o_3.png" alt="i_start=7">
            <img src="media/117___Text_Conditioned_Image_to_image_o_4.png" alt="i_start=10">
            <img src="media/118___Text_Conditioned_Image_to_image_o_5.png" alt="i_start=20">
            <img src="media/119___Text_Conditioned_Image_to_image_o_6.png" alt="Original">
        </div>

        <p><strong>Image of trees with prompt "an oil painting of an elderly bearded wizard":</strong></p>
        <div class="image-row">
            <img src="media/120___Text_Conditioned_Image_to_image_o_0.png" alt="i_start=1">
            <img src="media/121___Text_Conditioned_Image_to_image_o_1.png" alt="i_start=3">
            <img src="media/122___Text_Conditioned_Image_to_image_o_2.png" alt="i_start=5">
            <img src="media/123___Text_Conditioned_Image_to_image_o_3.png" alt="i_start=7">
            <img src="media/124___Text_Conditioned_Image_to_image_o_4.png" alt="i_start=10">
            <img src="media/125___Text_Conditioned_Image_to_image_o_5.png" alt="i_start=20">
            <img src="media/126___Text_Conditioned_Image_to_image_o_6.png" alt="Original">
        </div>

        <div class="section-divider"></div>

        <h3>1.8 Visual Anagrams</h3>
        <p>
            Visual anagrams look like one thing right-side up and another upside down.
            Created by averaging noise estimates from two prompts, one applied to the flipped image:
        </p>
        <p style="text-align: center;">
            <code>epsilon_1 = CFG(UNet(x_t, t, p1))</code><br>
            <code>epsilon_2 = flip(CFG(UNet(flip(x_t), t, p2)))</code><br>
            <code>epsilon = (epsilon_1 + epsilon_2) / 2</code>
        </p>

        <p><strong>Illusion 1: Wizard / Tree</strong></p>
        <div class="grid-2">
            <div>
                <img src="media/127_def_make_flip_illusion_prompt_embed_0.png" alt="Wizard">
                <p class="caption">Normal: "an oil painting of an elderly bearded wizard"</p>
            </div>
            <div>
                <img src="media/128_def_make_flip_illusion_prompt_embed_1.png" alt="Tree">
                <p class="caption">Flipped: "an oil painting of a twisted dead tree"</p>
            </div>
        </div>

        <p><strong>Illusion 2: Mountain / Waves</strong></p>
        <div class="grid-2">
            <div>
                <img src="media/129___Visual_Anagram_Illusion_2__Mounta_0.png" alt="Mountain">
                <p class="caption">Normal: "a watercolor of snowy mountain peaks"</p>
            </div>
            <div>
                <img src="media/130___Visual_Anagram_Illusion_2__Mounta_1.png" alt="Waves">
                <p class="caption">Flipped: "a watercolor of crashing ocean waves"</p>
            </div>
        </div>

        <div class="section-divider"></div>

        <h3>1.9 Hybrid Images</h3>
        <p>
            Hybrid images combine low frequencies of one image with high frequencies of another
            using Factorized Diffusion:
        </p>
        <p style="text-align: center;">
            <code>epsilon_1 = CFG(UNet(x_t, t, p1))</code><br>
            <code>epsilon_2 = CFG(UNet(x_t, t, p2))</code><br>
            <code>epsilon = lowpass(epsilon_1) + highpass(epsilon_2)</code>
        </p>
        <p>Using Gaussian blur with kernel size 33 and sigma 2 for the low-pass filter.</p>

        <p><strong>Hybrid 1: Tiger (low freq) + Fire (high freq)</strong></p>
        <div class="single-image">
            <img src="media/131_def_make_hybrids_prompt_embeds_1__p_0.png" alt="Tiger-Fire Hybrid">
            <p class="caption">Low: "a photo of a fierce tiger face" / High: "a photo of blazing orange fire"</p>
        </div>

        <p><strong>Hybrid 2: Moon (low freq) + Eye (high freq)</strong></p>
        <div class="single-image">
            <img src="media/132___Hybrid_Image_2__Moon__low_freq____0.png" alt="Moon-Eye Hybrid">
            <p class="caption">Low: "a painting of a giant full moon" / High: "a painting of a detailed human eye"</p>
        </div>

        <div class="section-divider"></div>

        <!-- ==================== PART B: FLOW MATCHING FROM SCRATCH ==================== -->

        <h2>Part B: Flow Matching from Scratch!</h2>
        <p>
            This part trains a flow matching model on MNIST from scratch.
        </p>

        <div class="section-divider"></div>

        <h3>Part 1: Training a Single-Step Denoising UNet</h3>

        <h4>1.1 Implementing the UNet</h4>
        <p>
            The UNet consists of downsampling and upsampling blocks with skip connections.
            It uses Conv, DownConv, UpConv, Flatten, and Unflatten operations. The architecture
            processes 28×28 MNIST images with hidden dimension D=128.
        </p>

        <h4>1.2 Using the UNet to Train a Denoiser</h4>
        <p>
            Noisy images are generated using <code>z = x + σε</code> where <code>ε ~ N(0, I)</code>.
            Below shows the noising process at different σ levels:
        </p>
        <div class="image-row">
            <img src="media/partb/noising_visualization.png" alt="Noising visualization" style="width: 100%; max-width: 800px; height: auto;">
        </div>
        <p class="caption">Noising process with σ = [0.0, 0.2, 0.4, 0.5, 0.6, 0.8, 1.0]</p>

        <h4>1.2.1 Training</h4>
        <p>
            The UNet is trained to denoise images with σ=0.5 using MSE loss, Adam optimizer (lr=1e-4),
            batch size 256, and hidden dimension D=128 for 5 epochs.
        </p>
        <div class="single-image">
            <img src="media/partb/training_loss_curve.png" alt="Training loss curve" style="width: 100%; max-width: 600px; height: auto;">
        </div>
        <p class="caption">Training loss curve for σ=0.5 denoising</p>

        <p><strong>Denoising Results:</strong></p>
        <div class="grid-2">
            <div>
                <img src="media/partb/denoising_epoch_1.png" alt="Epoch 1 results" style="width: 100%; height: auto;">
                <p class="caption">After Epoch 1</p>
            </div>
            <div>
                <img src="media/partb/denoising_epoch_5.png" alt="Epoch 5 results" style="width: 100%; height: auto;">
                <p class="caption">After Epoch 5</p>
            </div>
        </div>

        <h4>1.2.2 Out-of-Distribution Testing</h4>
        <p>
            The model (trained on σ=0.5) is tested on different noise levels.
        </p>
        <div class="single-image">
            <img src="media/partb/ood_testing.png" alt="OOD testing" style="width: 100%; max-width: 800px; height: auto;">
        </div>
        <p class="caption">OOD testing with σ = [0.0, 0.2, 0.4, 0.5, 0.6, 0.8, 1.0] (top: noisy input, bottom: denoised output)</p>

        <h4>1.2.3 Denoising Pure Noise</h4>
        <p>
            A model is trained to denoise pure Gaussian noise (z = ε where ε ~ N(0, I)).
        </p>
        <div class="single-image">
            <img src="media/partb/training_loss_pure_noise.png" alt="Pure noise training loss" style="width: 100%; max-width: 600px; height: auto;">
        </div>
        <p class="caption">Training loss curve for pure noise denoising</p>

        <div class="grid-2">
            <div>
                <img src="media/partb/pure_noise_epoch_1.png" alt="Pure noise epoch 1" style="width: 100%; height: auto;">
                <p class="caption">After Epoch 1</p>
            </div>
            <div>
                <img src="media/partb/pure_noise_epoch_5.png" alt="Pure noise epoch 5" style="width: 100%; height: auto;">
                <p class="caption">After Epoch 5</p>
            </div>
        </div>

        <p>
            The outputs resemble an average of training digits. With MSE loss, the model predicts
            the mean to minimize squared error. Since pure noise contains no digit information,
            the output is a blurry average rather than distinct digits.
        </p>

        <div class="section-divider"></div>

        <h3>Part 2: Training a Flow Matching Model</h3>
        <p>
            Flow matching trains a UNet to predict the flow from noisy to clean data.
            Linear interpolation: <code>x_t = (1-t)x_0 + t·x_1</code> where x_0 ~ N(0,I) and x_1 is clean data.
            The flow is <code>u(x_t, t) = x_1 - x_0</code>.
        </p>

        <h4>2.1 Adding Time Conditioning to UNet</h4>
        <p>
            Time conditioning is added using FCBlocks that inject the timestep t into the network.
            The conditioning is applied via multiplication: <code>unflatten = unflatten * t1</code> and <code>up1 = up1 * t2</code>.
        </p>

        <h4>2.2 Training the UNet</h4>
        <p>
            Training uses batch size 64, hidden dimension D=64, initial lr=1e-2 with ExponentialLR scheduler
            (γ = 0.1^(1/num_epochs)), for 10 epochs.
        </p>
        <div class="single-image">
            <img src="media/partb/time_fm_training_loss.png" alt="Time FM training loss" style="width: 100%; max-width: 600px; height: auto;">
        </div>
        <p class="caption">Training loss curve for time-conditioned flow matching</p>

        <h4>2.3 Sampling from the UNet</h4>
        <p>
            Sampling uses the Euler method: starting from pure noise x_0, iteratively apply
            <code>x_t = x_t + dt · u_θ(x_t, t)</code> for t from 0 to 1.
        </p>
        <div class="grid-3">
            <div>
                <img src="media/partb/time_fm_samples_epoch_1.png" alt="Time FM epoch 1" style="width: 100%; height: auto;">
                <p class="caption">Epoch 1</p>
            </div>
            <div>
                <img src="media/partb/time_fm_samples_epoch_5.png" alt="Time FM epoch 5" style="width: 100%; height: auto;">
                <p class="caption">Epoch 5</p>
            </div>
            <div>
                <img src="media/partb/time_fm_samples_epoch_10.png" alt="Time FM epoch 10" style="width: 100%; height: auto;">
                <p class="caption">Epoch 10</p>
            </div>
        </div>

        <h4>2.4 Adding Class-Conditioning to UNet</h4>
        <p>
            Class conditioning uses one-hot encoded digit labels.
            The conditioning combines time and class: <code>unflatten = c1 * unflatten + t1</code> and <code>up1 = c2 * up1 + t2</code>.
            10% dropout (p_uncond=0.1) is applied on class conditioning to enable CFG during sampling.
        </p>

        <h4>2.5 Training the UNet</h4>
        <p>
            Training uses the same hyperparameters as time-only, with class labels and random dropout.
        </p>
        <div class="single-image">
            <img src="media/partb/class_fm_training_loss.png" alt="Class FM training loss" style="width: 100%; max-width: 600px; height: auto;">
        </div>
        <p class="caption">Training loss curve for class-conditioned flow matching</p>

        <h4>2.6 Sampling from the UNet</h4>
        <p>
            Sampling uses CFG with γ=5.0: <code>u = u_uncond + γ(u_cond - u_uncond)</code>.
        </p>
        <div class="grid-3">
            <div>
                <img src="media/partb/class_fm_samples_epoch_1.png" alt="Class FM epoch 1" style="width: 100%; height: auto;">
                <p class="caption">Epoch 1</p>
            </div>
            <div>
                <img src="media/partb/class_fm_samples_epoch_5.png" alt="Class FM epoch 5" style="width: 100%; height: auto;">
                <p class="caption">Epoch 5</p>
            </div>
            <div>
                <img src="media/partb/class_fm_samples_epoch_10.png" alt="Class FM epoch 10" style="width: 100%; height: auto;">
                <p class="caption">Epoch 10</p>
            </div>
        </div>
        <p class="caption">4 instances of each digit (0-9) generated with class-conditioned flow matching</p>

        <p>
            Training without the ExponentialLR scheduler using compensation:
        </p>
        <ul>
            <li>Lower initial learning rate: 1e-3 instead of 1e-2</li>
            <li>More epochs: 15 instead of 10</li>
        </ul>
        <p>
            The original scheduler decays the LR by ~10× over training. Without it, a constant lower LR
            with more epochs achieves comparable results.
        </p>
        <div class="single-image">
            <img src="media/partb/no_sched_training_loss.png" alt="No scheduler training loss" style="width: 100%; max-width: 600px; height: auto;">
        </div>
        <p class="caption">Training loss curve without LR scheduler (constant lr=1e-3, 15 epochs)</p>

        <div class="grid-2">
            <div>
                <img src="media/partb/no_sched_samples_epoch_5.png" alt="No sched epoch 5" style="width: 100%; height: auto;">
                <p class="caption">Epoch 5</p>
            </div>
            <div>
                <img src="media/partb/no_sched_samples_epoch_15.png" alt="No sched epoch 15" style="width: 100%; height: auto;">
                <p class="caption">Epoch 15</p>
            </div>
        </div>
        <p class="caption">Samples from training without LR scheduler</p>

        <div class="section-divider"></div>
    </div>
</body>
</html>
